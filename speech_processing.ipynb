{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "import librosa \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 1\n",
    "## Creates segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed.csv\n",
    "processed_df = pd.read_csv(\"processed.csv\")\n",
    "processed_ids = set(processed_df['id'].astype(str))\n",
    "\n",
    "# Directory containing interview folders\n",
    "interview_root = \"interviews\"\n",
    "\n",
    "# Loop through each interview folder\n",
    "for interview_id in os.listdir(interview_root):\n",
    "    if interview_id in processed_ids:\n",
    "        print(f\"Interview {interview_id} already processed. Skipping...\")\n",
    "        continue  # Already processed\n",
    "\n",
    "    transcript_path = os.path.join(interview_root, interview_id, f\"{interview_id}_TRANSCRIPT.csv\")\n",
    "    audio_path = os.path.join(interview_root, interview_id, f\"{interview_id}_AUDIO.wav\")\n",
    "    output_audio_path = os.path.join(\"Segmented\", f\"{interview_id}_PATIENT.wav\")\n",
    "\n",
    "    # Check if both files exist\n",
    "    if not (os.path.exists(transcript_path) and os.path.exists(audio_path)):\n",
    "        print(f\"Missing files for interview {interview_id}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Load transcript\n",
    "        transcript = pd.read_csv(transcript_path, sep='\\t')\n",
    "        patient_segments = transcript[transcript[\"speaker\"] == \"Participant\"]\n",
    "\n",
    "        # Load full audio\n",
    "        audio = AudioSegment.from_wav(audio_path)\n",
    "\n",
    "        # Extract and combine patient segments\n",
    "        patient_audio = AudioSegment.silent()\n",
    "        for _, row in patient_segments.iterrows():\n",
    "            start_ms = int(row[\"start_time\"] * 1000)\n",
    "            stop_ms = int(row[\"stop_time\"] * 1000)\n",
    "            patient_audio += audio[start_ms:stop_ms]\n",
    "\n",
    "        # Export patient-only audio\n",
    "        patient_audio.export(output_audio_path, format=\"wav\")\n",
    "        print(f\"Processed interview {interview_id}.\")\n",
    "\n",
    "        # Optionally update processed.csv\n",
    "        processed_df.loc[len(processed_df)] = [interview_id]\n",
    "        processed_df.to_csv(\"processed.csv\", index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing interview {interview_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 2\n",
    "## Creates Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_dir = \"Segmented\"\n",
    "output_dir = \"Chunks\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "df = pd.read_csv('chunks.csv')\n",
    "chunked_files_id = df['id'].tolist()\n",
    "\n",
    "for file in os.listdir(segmented_dir):\n",
    "    if file.endswith('.wav'):\n",
    "        id = int(file.split('_')[0])\n",
    "    else:\n",
    "        continue \n",
    "    if id not in chunked_files_id:\n",
    "        print(f\"Processing file {file} for chunking...\")\n",
    "        audio = AudioSegment.from_wav(f\"Segmented/{file}\")\n",
    "        clip_length_ms = 8000\n",
    "        overlap_ms = 0\n",
    "        step_size_ms = clip_length_ms - overlap_ms\n",
    "        audio_clips = []\n",
    "        for start_ms in range(0,len(audio) - clip_length_ms + 1, step_size_ms):\n",
    "            clip = audio[start_ms:start_ms + clip_length_ms]\n",
    "            audio_clips.append(clip)\n",
    "            clip.export(f\"{output_dir}/{id}_clip_{start_ms // 1000}.wav\", format=\"wav\")\n",
    "\n",
    "    \n",
    "        new_row = {'id': id}  \n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        df.to_csv('chunks.csv', index=False)\n",
    "    else:\n",
    "        print(f\"File {file} already processed for chunking. Skipping...\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block 3\n",
    "## Creates Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output directories\n",
    "input_folder = 'Chunks'\n",
    "output_folder = 'Spectrograms'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "# Spectrogram parameters\n",
    "n_mels = 128  # Number of mel bands\n",
    "hop_length = 512  # Hop length for mel spectrogram\n",
    "n_fft = 2048  # FFT window size\n",
    "\n",
    "df = pd.read_csv('spectrograms.csv')\n",
    "spec_files_id = df['id'].astype(str).tolist()\n",
    "\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith('.wav'):\n",
    "        id = file.split('/')[0].split('.')[0]\n",
    "    else:\n",
    "        print(f\"Skipping non-WAV file: {file}\")\n",
    "        continue \n",
    "    if id not in spec_files_id:\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        print(file_path)\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        # Generate mel spectrogram\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "        # Normalize spectrogram (scale to [0, 1])\n",
    "        S_dB = (S_dB - S_dB.min()) / (S_dB.max() - S_dB.min())\n",
    "\n",
    "        # Save as NumPy array\n",
    "        out_file = os.path.join(output_folder, f\"{id}.npy\")\n",
    "        np.save(out_file, S_dB)\n",
    "\n",
    "        # Update spectrograms.csv\n",
    "        new_row = {'id': id}  \n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        df.to_csv('spectrograms.csv', index=False)\n",
    "       \n",
    "    else:\n",
    "        print(f\"Already created spectrograms for file : {file} . Skipping...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spectrogram array\n",
    "array = np.load('Spectrograms/373_clip_0.wav.npy')\n",
    "# Create the spectrogram image\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(array, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.colorbar(label='Normalized Amplitude')\n",
    "plt.title('Spectrogram of 373_clip_0')\n",
    "plt.xlabel('Time Frames')\n",
    "plt.ylabel('Mel Bands')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merged Phq9 from 3 different csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('documents/dev_split_Depression_AVEC2017.csv')\n",
    "df2 = pd.read_csv('documents/full_test_split.csv')\n",
    "df3 = pd.read_csv('documents/train_split_Depression_AVEC2017.csv')\n",
    "\n",
    "# Extract the required columns from each DataFrame\n",
    "df1_subset = df1[['Participant_ID', 'PHQ8_Binary', 'PHQ8_Score', 'Gender']]\n",
    "df2_subset = df2[['Participant_ID', 'PHQ8_Binary', 'PHQ8_Score', 'Gender']]\n",
    "df3_subset = df3[['Participant_ID', 'PHQ8_Binary', 'PHQ8_Score', 'Gender']]\n",
    "\n",
    "# Combine the subsets into a single DataFrame\n",
    "combined_df = pd.concat([df1_subset, df2_subset, df3_subset], ignore_index=True)\n",
    "\n",
    "# Sort the combined DataFrame by Participant_ID\n",
    "combined_df = combined_df.sort_values(by='Participant_ID')\n",
    "\n",
    "# Save the sorted DataFrame to a new CSV file\n",
    "combined_df.to_csv('phq8.csv', index=False)\n",
    "print(\"New CSV file created: phq8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset.csv needed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('phq8.csv')\n",
    "spectrograms_dir = 'Spectrograms'\n",
    "dataset_df = pd.read_csv('dataset.csv')\n",
    "for file in os.listdir(spectrograms_dir):\n",
    "    spec_id = file.split('.')[0]\n",
    "    patient_id = int(spec_id.split('_')[0])\n",
    "    if patient_id in df['Participant_ID'].values:\n",
    "        phq8_binary = df.loc[df['Participant_ID'] == patient_id, 'PHQ8_Binary'].values\n",
    "        new_row = {'id': spec_id, 'label' : phq8_binary[0]}  \n",
    "        dataset_df = pd.concat([dataset_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        dataset_df.to_csv('dataset.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms_dir = 'Spectrograms'\n",
    "for file in os.listdir(spectrograms_dir):\n",
    "    arr = np.load(os.path.join(spectrograms_dir, file))\n",
    "    if arr.shape != (128, 251):\n",
    "        print(f\"Error: {file} has an invalid shape. Expected (128, 251), got {arr.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "print(len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
