{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, spectrogram_folder, label_csv):\n",
    "        self.spectrogram_folder = spectrogram_folder\n",
    "        self.df = pd.read_csv(label_csv)\n",
    "        self.ids = self.df['id'].astype(str).tolist()\n",
    "        self.labels = self.df['label'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load spectrogram\n",
    "        spec_path = os.path.join(self.spectrogram_folder, f\"{self.ids[idx]}.npy\")\n",
    "        spectrogram = np.load(spec_path)\n",
    "        # Add channel dimension (1, n_mels, time_steps) for CNN\n",
    "        spectrogram = spectrogram[np.newaxis, :, :]\n",
    "        spectrogram = torch.tensor(spectrogram, dtype=torch.float32)\n",
    "        # Get label\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_folder = 'Spectrograms'\n",
    "label_csv = 'dataset.csv'  # CSV with 'id' and 'label' columns\n",
    "dataset = SpectrogramDataset(spectrogram_folder, label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNDNN, self).__init__()\n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Correct flatten size based on input (1, 128, 251)\n",
    "        self.flatten_size = 128 * (128 // 8) * (251 // 8)  # = 128 * 16 * 31 = 63,488\n",
    "        \n",
    "        # DNN layers\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.batchnorm1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.batchnorm2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.batchnorm3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNNDNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 3.0611\n",
      "Validation Accuracy: 58.36%\n",
      "Epoch [2/20], Loss: 0.8062\n",
      "Validation Accuracy: 57.03%\n",
      "Epoch [3/20], Loss: 0.7015\n",
      "Validation Accuracy: 57.03%\n",
      "Epoch [4/20], Loss: 0.6914\n",
      "Validation Accuracy: 57.03%\n",
      "Epoch [5/20], Loss: 0.6818\n",
      "Validation Accuracy: 57.03%\n",
      "Epoch [6/20], Loss: 0.6739\n",
      "Validation Accuracy: 57.03%\n",
      "Epoch [7/20], Loss: 0.6692\n",
      "Validation Accuracy: 58.62%\n",
      "Epoch [8/20], Loss: 0.6667\n",
      "Validation Accuracy: 58.36%\n",
      "Epoch [9/20], Loss: 0.6658\n",
      "Validation Accuracy: 61.27%\n",
      "Epoch [10/20], Loss: 0.6659\n",
      "Validation Accuracy: 60.21%\n",
      "Epoch [11/20], Loss: 0.6463\n",
      "Validation Accuracy: 63.13%\n",
      "Epoch [12/20], Loss: 0.6610\n",
      "Validation Accuracy: 61.54%\n",
      "Epoch [13/20], Loss: 0.6347\n",
      "Validation Accuracy: 63.13%\n",
      "Epoch [14/20], Loss: 0.6368\n",
      "Validation Accuracy: 61.27%\n",
      "Epoch [15/20], Loss: 0.6657\n",
      "Validation Accuracy: 61.27%\n",
      "Epoch [16/20], Loss: 0.6328\n",
      "Validation Accuracy: 58.62%\n",
      "Epoch [17/20], Loss: 0.6267\n",
      "Validation Accuracy: 62.86%\n",
      "Epoch [18/20], Loss: 0.6213\n",
      "Validation Accuracy: 64.46%\n",
      "Epoch [19/20], Loss: 0.6261\n",
      "Validation Accuracy: 62.86%\n",
      "Epoch [20/20], Loss: 0.6314\n",
      "Validation Accuracy: 63.13%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for spectrograms, labels in train_loader:\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(spectrograms)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation (optional)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels in test_loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 63.13%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for spectrograms, labels in test_loader:\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        outputs = model(spectrograms)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "# torch.save(model.state_dict(), 'cnn_dnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model later\n",
    "model = CNNDNN().to(device)\n",
    "model.load_state_dict(torch.load('cnn_dnn_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Considerations\n",
    "# 1. Data Augmentation: To improve generalization, apply augmentations like random cropping, noise addition, or time/frequency masking to the spectrograms.\n",
    "# 2. Use libraries like torchaudio for audio-specific augmentations.\n",
    "# 3. Class Imbalance: If your dataset is imbalanced (e.g., more non-depressed than depressed samples), use a weighted loss function:\n",
    "\n",
    "# class_weights = torch.tensor([1.0, 5.0]).to(device)  # Adjust weights based on class distribution\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# 4. Hyperparameter Tuning: Experiment with learning rate, batch size, number of CNN/DNN layers, and dropout rate.\n",
    "# 5. Early Stopping: Monitor validation loss and stop training if it stops improving to prevent overfitting.\n",
    "# 6. GPU Usage: Ensure you have CUDA installed if using a GPU for faster training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single spectrogram\n",
    "spec_path = 'Spectrograms/sample.npy'\n",
    "spectrogram = np.load(spec_path)\n",
    "spectrogram = spectrogram[np.newaxis, np.newaxis, :, :]  # Shape: (1, 1, n_mels, time_steps)\n",
    "spectrogram = torch.tensor(spectrogram, dtype=torch.float32).to(device)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(spectrogram)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    print(f\"Predicted class: {'Depressed' if predicted.item() == 1 else 'Not Depressed'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
